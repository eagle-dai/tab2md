import asyncio
import os
import re
import sys
import subprocess
from pathlib import Path
from playwright.async_api import async_playwright
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode

# === Configuration ===
# å¼ºåˆ¶ä½¿ç”¨ IPv4 127.0.0.1 é¿å… Windows ä¸‹çš„ IPv6 é—®é¢˜
DEBUG_PORT_URL = "http://127.0.0.1:9222"
OUTPUT_DIR = "exports"


def ensure_chromium_installed():
    """Check and auto-install Chromium if needed."""
    try:
        subprocess.run(
            ["playwright", "install", "chromium"],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE,
        )
    except (subprocess.CalledProcessError, FileNotFoundError):
        pass  # Silently fail or rely on user manual install


async def get_active_tab_snapshot():
    """
    Connect to the running browser (Edge/Chrome) via CDP
    and capture the DOM of the active tab.
    """
    try:
        async with async_playwright() as p:
            try:
                # å°è¯•è¿æ¥åˆ°è°ƒè¯•ç«¯å£
                browser = await p.chromium.connect_over_cdp(DEBUG_PORT_URL)
            except Exception:
                print(f"âŒ Connection Failed: Could not connect to {DEBUG_PORT_URL}")
                print(
                    "âš ï¸  Ensure your browser is started with: --remote-debugging-port=9222"
                )
                return None, None

            if not browser.contexts:
                print("âŒ No browser context found.")
                await browser.close()
                return None, None

            ctx = browser.contexts[0]
            pages = ctx.pages

            if not pages:
                print("âŒ No pages found in browser.")
                await browser.close()
                return None, None

            # === [æ ¸å¿ƒé€»è¾‘ä¼˜åŒ–] å¯»æ‰¾å½“å‰æ¿€æ´»çš„ Tab ===
            target_page = None
            fallback_page = None  # ç”¨äºå…œåº•

            print(f"ğŸ” Scanning {len(pages)} tabs for the active one...")

            for page in pages:
                # 1. åŸºç¡€è¿‡æ»¤ï¼šè·³è¿‡ç©ºç™½é¡µå’Œ DevTools
                if page.url == "about:blank" or page.url.startswith("devtools://"):
                    continue

                # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é¡µé¢ä½œä¸ºå…œåº•
                if fallback_page is None:
                    fallback_page = page

                try:
                    # 2. è¯¢é—®é¡µé¢çŠ¶æ€ï¼šåªæœ‰å½“å‰æ¿€æ´»çš„ Tab çŠ¶æ€ä¸º 'visible'
                    visibility = await page.evaluate("document.visibilityState")

                    if visibility == "visible":
                        target_page = page
                        print("âœ… Found active tab (visible).")
                        break
                except Exception:
                    continue

            # å¦‚æœæ²¡æ‰¾åˆ° visible çš„ï¼Œä½¿ç”¨å…œåº•é¡µé¢
            if not target_page:
                if fallback_page:
                    print("âš ï¸ No visible tab found, using the first valid tab.")
                    target_page = fallback_page
                else:
                    print("âŒ No valid web page found.")
                    await browser.close()
                    return None, None

            # è·å–ä¿¡æ¯
            title = await target_page.title()
            print(f"ğŸ”— Targeted Tab: {title}")
            print(f"ğŸ”— URL: {target_page.url}")

            # æŠ“å–å®Œæ•´æ¸²æŸ“åçš„ HTML
            content = await target_page.content()
            url = target_page.url

            # ä½¿ç”¨ close() æ–­å¼€è¿æ¥ (ä¸ä¼šå…³é—­ Edge çª—å£)
            await browser.close()

            return url, content

    except Exception as e:
        print(f"ğŸ”¥ Error during snapshot: {e}")
        return None, None


def inject_base_tag(html: str, url: str) -> str:
    """Inject <base> tag to fix relative links."""
    base_tag = f'<base href="{url}">'
    if "<head>" in html:
        return html.replace("<head>", f"<head>\n{base_tag}", 1)
    return f"<html><head>{base_tag}</head>" + html


async def process_conversion():
    # 1. Capture Snapshot
    url, raw_html = await get_active_tab_snapshot()
    if not raw_html:
        return

    # 2. Prepare Local File
    html_with_base = inject_base_tag(raw_html, url)
    temp_file = Path("temp_snapshot.html").resolve()
    temp_file.write_text(html_with_base, encoding="utf-8")

    # === [Windows è·¯å¾„å…¼å®¹æ€§] ===
    local_file_uri = f"file://{temp_file.as_posix()}"

    print("ğŸš€ Running extraction engine (Crawl4AI)...")

    # 3. Configure Extraction
    browser_cfg = BrowserConfig(headless=True, verbose=False)
    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        magic=True,
        word_count_threshold=5,
        excluded_tags=[
            "nav",
            "footer",
            "aside",
            "script",
            "style",
            "iframe",
            "form",
            "noscript",
            "svg",
        ],
    )

    # 4. Run Conversion
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(url=local_file_uri, config=run_cfg)

        if result.success:
            # Generate Safe Filename
            slug = re.sub(r"[^a-zA-Z0-9]", "_", url.split("//")[-1])
            safe_name = f"{slug[:50]}"

            output_path = Path(OUTPUT_DIR)
            output_path.mkdir(exist_ok=True)

            md_file = output_path / f"{safe_name}.md"
            md_file.write_text(result.markdown, encoding="utf-8")

            print(f"\nâœ… Conversion Complete!")
            print(f"ğŸ“‚ Saved to: {md_file}")

            # === Debug: ä¿ç•™ä¸´æ—¶æ–‡ä»¶ (å¦‚ä¸éœ€è¦å¯å–æ¶ˆæ³¨é‡Šä¸‹æ–¹ä»£ç è¿›è¡Œåˆ é™¤) ===
            # try:
            #     os.remove(temp_file)
            # except:
            #     pass
            print(f"ğŸ› Debug: Snapshot kept at {temp_file}")
        else:
            print(f"âŒ Conversion Failed: {result.error_message}")


def entry_point():
    ensure_chromium_installed()
    asyncio.run(process_conversion())


if __name__ == "__main__":
    entry_point()
